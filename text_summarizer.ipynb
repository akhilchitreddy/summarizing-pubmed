{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda4ef37-0b98-40af-8bcc-05ac1c96d7dd",
   "metadata": {},
   "source": [
    "# Summarizing Pubmed Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d7a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#install necessary libraries\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aadff603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import json \n",
    "import warnings \n",
    "import tabulate\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from nltk import word_tokenize\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59b3818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_seed(102)\n",
    "\n",
    "#load evaluation metric\n",
    "METRIC = load(\"rouge\")\n",
    "\n",
    "# Define device type\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "DEVICE_NAME=\"cuda\"\n",
    "\n",
    "# Define constants\n",
    "NUMBER_OF_RECORDS = 100\n",
    "MAX_SUMMARY_LENGTH = 200\n",
    "MAX_TOKEN_SIZE = 512\n",
    "NUMBER_OF_CHARACTERS = 100\n",
    "INPUT_DATA_PATH = './pub_data.txt'\n",
    "SENTENCE_TRANSFORMER_MODEL = 'pritamdeka/S-PubMedBert-MS-MARCO'\n",
    "\n",
    "#define empty dictionaries for appending metrics\n",
    "ROUGE_METRICS = {}\n",
    "TIME_METRICS = {}\n",
    "\n",
    "# Define text splitters by token size\n",
    "TEXT_SPLITTER=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=MAX_TOKEN_SIZE, chunk_overlap=NUMBER_OF_CHARACTERS)\n",
    "\n",
    "#Different versions of transformer based models\n",
    "GPT_SUMMARIZATION_MODELS = ['gpt2','gpt2-medium']\n",
    "T5_SUMMARIZATION_MODELS = ['t5-base','t5-small','gayanin/t5-small-finetuned-pubmed']\n",
    "BART_SUMMARIZATION_MODELS = ['facebook/bart-base','facebook/bart-large','mse30/bart-base-finetuned-pubmed']\n",
    "PEGASUS_SUMMARIZATION_MODELS = ['google/pegasus-x-base','google/pegasus-x-large','google/pegasus-pubmed','google/bigbird-pegasus-large-pubmed']\n",
    "ALL_MODELS = GPT_SUMMARIZATION_MODELS+T5_SUMMARIZATION_MODELS+BART_SUMMARIZATION_MODELS+PEGASUS_SUMMARIZATION_MODELS\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037900ed-de09-4470-98fd-b2dcd5d08da8",
   "metadata": {},
   "source": [
    "## Load Pubmed Dataset\n",
    "Load data into a dataframe with article_id, article_text and article_text_raw as attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6725c785-6b05-466c-8868-d9e796dbc926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path:str=INPUT_DATA_PATH)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Pubmed dataset to dataframe\n",
    "    \"\"\"\n",
    "    pubmed_data={}\n",
    "    article_id=\"\"\n",
    "    article_text=\"\"\n",
    "    print(\"############# Started Data Loading ##############\")\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            article_id = json.loads(line)['article_id']\n",
    "            article_text_raw = json.loads(line)['article_text']\n",
    "            article_text = \" \".join(article_text_raw)\n",
    "            abstract_text=\"\".join(x.strip(\"<S> </S>\") for x in json.loads(line)['abstract_text'])\n",
    "            pubmed_data[article_id]=[article_text,abstract_text,article_text_raw]\n",
    "    df=pd.DataFrame.from_dict(pubmed_data,orient='index',columns=['article_text','abstract_text','article_text_raw'])\n",
    "    print(\"############# Finished Data Loading ##############\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cd830-89b8-4f48-9bf9-65b2a7bec641",
   "metadata": {},
   "source": [
    "## Chunker Class to generate normal and semantic chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1248a17f-351a-4ca0-b8a3-ee9f10ea0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class chunker():\n",
    "\n",
    "    def get_pubmed_embeddings(self,sentence_list:list)->list:\n",
    "        \"\"\"\n",
    "        Get the list of embeddings from pretrained model on pubmed data for the sentences\n",
    "        \"\"\"\n",
    "        sentence_model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL)\n",
    "        embeddings = sentence_model.encode(sentence_list)\n",
    "        return embeddings\n",
    "\n",
    "    def calculate_cosine_distances(self,sentence_list:list) -> tuple[list,list]:\n",
    "        \"\"\"\n",
    "        Find the cosine distances between adjacent sentences\n",
    "        \"\"\"\n",
    "        distances = []\n",
    "        for i in range(len(sentence_list) - 1):\n",
    "            embedding_current = sentence_list[i]['combined_sentence_embedding']\n",
    "            embedding_next = sentence_list[i + 1]['combined_sentence_embedding']\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "            \n",
    "            # Convert to cosine distance\n",
    "            distance = 1 - similarity\n",
    "    \n",
    "            # Append cosine distance to the list\n",
    "            distances.append(distance)\n",
    "    \n",
    "            # Store distance in the dictionary\n",
    "            sentence_list[i]['distance_to_next'] = distance\n",
    "    \n",
    "        return distances, sentence_list\n",
    "\n",
    "    def combine_sentences(self,sentence_list:list, buffer_size:int=1)->list:\n",
    "        \"\"\"\n",
    "        Combine buffer size number of adjacent sentences into a single sentence \n",
    "        \"\"\"\n",
    "        # Go through each sentence dict\n",
    "        for i in range(len(sentence_list)):\n",
    "    \n",
    "            # Create a string that will hold the sentences which are joined\n",
    "            combined_sentence = ''\n",
    "    \n",
    "            # Add sentences before the current one, based on the buffer size.\n",
    "            for j in range(i - buffer_size, i):\n",
    "                # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "                if j >= 0:\n",
    "                    # Add the sentence at index j to the combined_sentence string\n",
    "                    combined_sentence += sentence_list[j]['sentence'] + ' '\n",
    "    \n",
    "            # Add the current sentence\n",
    "            combined_sentence += sentence_list[i]['sentence']\n",
    "    \n",
    "            # Add sentences after the current one, based on the buffer size\n",
    "            for j in range(i + 1, i + 1 + buffer_size):\n",
    "                # Check if the index j is within the range of the sentences list\n",
    "                if j < len(sentence_list):\n",
    "                    # Add the sentence at index j to the combined_sentence string\n",
    "                    combined_sentence += ' ' + sentence_list[j]['sentence']\n",
    "    \n",
    "            # Then add the whole thing to your dict\n",
    "            # Store the combined sentence in the current sentence dict\n",
    "            sentence_list[i]['combined_sentence'] = combined_sentence\n",
    "    \n",
    "        return sentence_list\n",
    "\n",
    "    def create_semantic_chunks(self,article_text_list:list)->list:\n",
    "        \"\"\"\n",
    "        Takes in list of sentences of the article and creates semantic chunks of them\n",
    "        \"\"\"\n",
    "        breakpoint_percentile_threshold = 95\n",
    "        #single_sentences_list = re.split(r'(?<=\\.)\\s+', article_text)\n",
    "        sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(article_text_list)]\n",
    "        sentences = self.combine_sentences(sentences)\n",
    "        embeddings = self.get_pubmed_embeddings([x['combined_sentence'] for x in sentences])\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "        distances, sentences = self.calculate_cosine_distances(sentences)\n",
    "        breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
    "        print(breakpoint_distance_threshold)\n",
    "        indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "        # Initialize the start index\n",
    "        start_index = 0\n",
    "        \n",
    "        # Create a list to hold the grouped sentences\n",
    "        chunks = []\n",
    "        \n",
    "        # Iterate through the breakpoints to slice the sentences\n",
    "        for index in indices_above_thresh:\n",
    "            # The end index is the current breakpoint\n",
    "            end_index = index\n",
    "        \n",
    "            # Slice the sentence_dicts from the current start index to the end index\n",
    "            group = sentences[start_index:end_index + 1]\n",
    "            group_sentence_list = [d['sentence'] for d in group]\n",
    "            combined_text = ' '.join(group_sentence_list)\n",
    "            # call the function create_semantic_chunks recursively if the chunk size is greater than MAX_TOKEN_SIZE\n",
    "            if len(word_tokenize(combined_text)) > MAX_TOKEN_SIZE:\n",
    "                #chunks = chunks + TEXT_SPLITTER.split_text(combined_text)\n",
    "                chunks = chunks + self.create_semantic_chunks(group_sentence_list)\n",
    "            else:\n",
    "                chunks.append(combined_text)\n",
    "            \n",
    "            # Update the start index for the next group\n",
    "            start_index = index + 1\n",
    "        \n",
    "        # The last group, if any sentences remain\n",
    "        if start_index < len(sentences):\n",
    "            group_sentence_list = [d['sentence'] for d in sentences[start_index:]]\n",
    "            combined_text = ' '.join(group_sentence_list)\n",
    "            # call the function create_semantic_chunks recursively if the chunk size is greater than MAX_TOKEN_SIZE\n",
    "            if len(word_tokenize(combined_text)) > MAX_TOKEN_SIZE:\n",
    "                #chunks = chunks + TEXT_SPLITTER.split_text(combined_text)\n",
    "                chunks = chunks + self.create_semantic_chunks(group_sentence_list)\n",
    "            else:\n",
    "                chunks.append(combined_text)\n",
    "        return chunks\n",
    "\n",
    "    def get_chunks(self,df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        creates a new column with the normal chunks of the article text for every record\n",
    "        \"\"\"\n",
    "        print(\"############# Generating Chunks ##############\")\n",
    "        df['chunks'] = df['article_text'].apply(lambda x:TEXT_SPLITTER.split_text(x))\n",
    "        print(\"############# Completed Generating Chunks ##############\")\n",
    "        return df\n",
    "        \n",
    "    def get_semantic_chunks(self,df:pd.DataFrame)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        creates a new column with the semantic chunks of the article text for every record\n",
    "        \"\"\"\n",
    "        print(\"############# Generating Semantic Chunks ##############\")\n",
    "        df['semantic_chunks'] = df['article_text_raw'].apply(lambda x:self.create_semantic_chunks(x))\n",
    "        print(\"############# Completed Generating Semantic Chunks ##############\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf8dfe-9f08-4be1-8eb8-888b57a952a6",
   "metadata": {},
   "source": [
    "## Summarization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ccc9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_summarizer():\n",
    "    # define variable to use fast tokenization\n",
    "    USE_FAST=True\n",
    "\n",
    "    def get_summary_gpt(self,model:str,chunks:list)->str:\n",
    "        \"\"\"\n",
    "        Takes in the gpt model and the list of chunks to generate the output summary\n",
    "        \"\"\"\n",
    "        final_summary_list=[]\n",
    "        # define pipeline for generating summary\n",
    "        summarizer = pipeline('text-generation', model = model, device=DEVICE)\n",
    "        # iterate over chunks to generate summary\n",
    "        for chunk in chunks:\n",
    "            query = chunk + \"\\nTL;DR:\\n\"\n",
    "            pipe_out = summarizer(query,max_new_tokens=MAX_SUMMARY_LENGTH,clean_up_tokenization_spaces=True)\n",
    "            # skip the input chunk and also the 8 characters \\nTL;DR:\\n at the end to get the summary\n",
    "            final_summary_list.append(pipe_out[0]['generated_text'][len(chunk)+8:])\n",
    "        # combine all the summaries into one\n",
    "        final_summary = ''.join(final_summary_list)\n",
    "        # if final summary is greater than 512 tokens, recursively summarize \n",
    "        if len(word_tokenize(final_summary))>MAX_TOKEN_SIZE:\n",
    "            return self.get_summary_gpt(model,TEXT_SPLITTER.split_text(final_summary))\n",
    "        # finally summarize the combined summary\n",
    "        output_summary = summarizer(final_summary,max_new_tokens=MAX_SUMMARY_LENGTH,clean_up_tokenization_spaces=True)\n",
    "        # skip the input chunk and also the 8 characters \\nTL;DR:\\n at the end to get the summary\n",
    "        return output_summary[0]['generated_text'][len(chunk)+8:]\n",
    "    \n",
    "    def get_summary_others(self,model:str,chunks:list)->str:\n",
    "        \"\"\"\n",
    "        Takes in the other model and the list of chunks to generate the output summary\n",
    "        \"\"\"\n",
    "        final_summary_list=[]\n",
    "        # disable fast tokenization as it is not compatible with google/pegasus-pubmed\n",
    "        if model == PEGASUS_SUMMARIZATION_MODELS[2]:\n",
    "            self.USE_FAST=False\n",
    "        # define pipeline for generating summary\n",
    "        summarizer = pipeline('summarization', model = model, device=DEVICE,use_fast=self.USE_FAST)\n",
    "        # iterate over chunks to generate summary\n",
    "        for chunk in chunks:\n",
    "            query = chunk\n",
    "            pipe_out = summarizer(query,max_new_tokens=MAX_SUMMARY_LENGTH,clean_up_tokenization_spaces=True)\n",
    "            final_summary_list.append(pipe_out[0]['summary_text'])\n",
    "        # combine all the summaries into one\n",
    "        final_summary = ''.join(final_summary_list)\n",
    "        # if final summary is greater than 512 tokens, recursively summarize \n",
    "        if len(word_tokenize(final_summary))>MAX_TOKEN_SIZE:\n",
    "            return self.get_summary(model,TEXT_SPLITTER.split_text(final_summary))\n",
    "        # finally summarize the combined summary\n",
    "        output_summary = summarizer(final_summary,max_new_tokens=MAX_SUMMARY_LENGTH,clean_up_tokenization_spaces=True)\n",
    "        return output_summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80836525-df3c-4658-b936-dddef9693003",
   "metadata": {},
   "source": [
    "## Predictions Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c079999f-8dd4-4774-9d52-4890bfa4c258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class run_predictions():\n",
    "\n",
    "    def run_summarization_models(self,data:pd.DataFrame,model_list:list=GPT_SUMMARIZATION_MODELS,chunking_type:str=\"chunks\")->tuple[pd.DataFrame,pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Takes the dataframe, list of models and chunking type as the inputs and stores the predictions in a new column in the dataframe\n",
    "        \"\"\"\n",
    "        # iterate through all the models to run the predictions\n",
    "        for model in model_list:\n",
    "            print(\"############# Summarizing using {} ##############\".format(model))\n",
    "            start = time.time()\n",
    "            if model.startswith('gpt'):\n",
    "                data[model+'_'+chunking_type+'_predictions']=data[chunking_type].apply(lambda chunks:text_summarizer().get_summary_gpt(model,chunks))\n",
    "            else:\n",
    "                data[model+'_'+chunking_type+'_predictions']=data[chunking_type].apply(lambda chunks:text_summarizer().get_summary_others(model,chunks))\n",
    "            end = time.time()\n",
    "            print(\"############# Predictions completed for {} ############## \\n\".format(model))\n",
    "            print(\"############# Logging Metrics ##############\")\n",
    "            ROUGE_METRICS[model+'_'+chunking_type+'_predictions']=METRIC.compute(predictions=data[model+'_'+chunking_type+'_predictions'], references=data['abstract_text'])\n",
    "            TIME_METRICS[model+'_'+chunking_type+'_predictions']=round(end-start, 2)\n",
    "            print(\"############# Logging Completed ############## \\n\")\n",
    "        ROUGE_METRICS = pd.DataFrame.from_dict(ROUGE_METRICS,orient='index').sort_values(by=['rouge1'],ascending=False)\n",
    "        TIME_METRICS = pd.DataFrame.from_dict(TIME_METRICS,columns=['time(seconds)'],orient='index').sort_values(by=['time(seconds)'],ascending=False)\n",
    "        return ROUGE_METRICS, TIME_METRICS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d43aef-0005-44bb-a118-a2e5634e793b",
   "metadata": {},
   "source": [
    "## Running all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe1bf90-f881-41e5-8a36-d99801c1b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Started Data Loading ##############\n",
      "############# Finished Data Loading ##############\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 452 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pubmed_df=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5c868e9-8aec-4ea4-a23b-7e692c42b30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 30.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Randomly select 100 records\n",
    "pubmed_df_N=pubmed_df.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47b7e0ce-993a-4625-b436-a23872e48602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Generating Chunks ##############\n",
      "############# Completed Generating Chunks ##############\n",
      "CPU times: total: 3.45 s\n",
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pubmed_df_N = chunker().get_chunks(pubmed_df_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b518c-6f1a-4749-beba-43a828e31a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pubmed_df_N = chunker().get_semantic_chunks(pubmed_df_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7356f-ea0c-4d3f-885a-2a85320f07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ROUGE_METRICS,TIME_METRICS=run_predictions().run_summarization_models(data=pubmed_df_N,model_list=ALL_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72739f15-4aef-4c96-9294-5780f3294610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google/bigbird-pegasus-large-pubmed_predictions</th>\n",
       "      <td>0.388646</td>\n",
       "      <td>0.154228</td>\n",
       "      <td>0.232430</td>\n",
       "      <td>0.237661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-pubmed_predictions</th>\n",
       "      <td>0.383846</td>\n",
       "      <td>0.152244</td>\n",
       "      <td>0.232697</td>\n",
       "      <td>0.237762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-pubmed_semantic_predictions</th>\n",
       "      <td>0.379763</td>\n",
       "      <td>0.155475</td>\n",
       "      <td>0.236825</td>\n",
       "      <td>0.240366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/bigbird-pegasus-large-pubmed_semantic_predictions</th>\n",
       "      <td>0.375570</td>\n",
       "      <td>0.139944</td>\n",
       "      <td>0.223682</td>\n",
       "      <td>0.227538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse30/bart-base-finetuned-pubmed_predictions</th>\n",
       "      <td>0.371113</td>\n",
       "      <td>0.146808</td>\n",
       "      <td>0.228494</td>\n",
       "      <td>0.252258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse30/bart-base-finetuned-pubmed_semantic_predictions</th>\n",
       "      <td>0.355427</td>\n",
       "      <td>0.140262</td>\n",
       "      <td>0.220893</td>\n",
       "      <td>0.246486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-base_semantic_predictions</th>\n",
       "      <td>0.337382</td>\n",
       "      <td>0.113309</td>\n",
       "      <td>0.192816</td>\n",
       "      <td>0.196912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-base_predictions</th>\n",
       "      <td>0.334315</td>\n",
       "      <td>0.114154</td>\n",
       "      <td>0.192467</td>\n",
       "      <td>0.196672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large_semantic_predictions</th>\n",
       "      <td>0.314529</td>\n",
       "      <td>0.091228</td>\n",
       "      <td>0.172488</td>\n",
       "      <td>0.176876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large_predictions</th>\n",
       "      <td>0.309903</td>\n",
       "      <td>0.086964</td>\n",
       "      <td>0.172292</td>\n",
       "      <td>0.175652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-x-large_predictions</th>\n",
       "      <td>0.301847</td>\n",
       "      <td>0.086326</td>\n",
       "      <td>0.163246</td>\n",
       "      <td>0.169236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-x-large_semantic_predictions</th>\n",
       "      <td>0.289379</td>\n",
       "      <td>0.086078</td>\n",
       "      <td>0.157694</td>\n",
       "      <td>0.161319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-medium_predictions</th>\n",
       "      <td>0.188879</td>\n",
       "      <td>0.019092</td>\n",
       "      <td>0.099106</td>\n",
       "      <td>0.120994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2_predictions</th>\n",
       "      <td>0.184496</td>\n",
       "      <td>0.017993</td>\n",
       "      <td>0.099412</td>\n",
       "      <td>0.120907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5-small_predictions</th>\n",
       "      <td>0.181466</td>\n",
       "      <td>0.062812</td>\n",
       "      <td>0.129782</td>\n",
       "      <td>0.130728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2_semantic_predictions</th>\n",
       "      <td>0.176464</td>\n",
       "      <td>0.016244</td>\n",
       "      <td>0.099743</td>\n",
       "      <td>0.119662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5-small_semantic_predictions</th>\n",
       "      <td>0.173886</td>\n",
       "      <td>0.054415</td>\n",
       "      <td>0.119856</td>\n",
       "      <td>0.121929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5-base_predictions</th>\n",
       "      <td>0.169984</td>\n",
       "      <td>0.057185</td>\n",
       "      <td>0.122764</td>\n",
       "      <td>0.123243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5-base_semantic_predictions</th>\n",
       "      <td>0.161036</td>\n",
       "      <td>0.048650</td>\n",
       "      <td>0.112587</td>\n",
       "      <td>0.113431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gayanin/t5-small-finetuned-pubmed_predictions</th>\n",
       "      <td>0.127654</td>\n",
       "      <td>0.049265</td>\n",
       "      <td>0.100367</td>\n",
       "      <td>0.101464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gayanin/t5-small-finetuned-pubmed_semantic_predictions</th>\n",
       "      <td>0.117538</td>\n",
       "      <td>0.041906</td>\n",
       "      <td>0.092779</td>\n",
       "      <td>0.093721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-x-base_semantic_predictions</th>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>0.025338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-x-base_predictions</th>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>0.021687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      rouge1    rouge2  \\\n",
       "google/bigbird-pegasus-large-pubmed_predictions     0.388646  0.154228   \n",
       "google/pegasus-pubmed_predictions                   0.383846  0.152244   \n",
       "google/pegasus-pubmed_semantic_predictions          0.379763  0.155475   \n",
       "google/bigbird-pegasus-large-pubmed_semantic_pr...  0.375570  0.139944   \n",
       "mse30/bart-base-finetuned-pubmed_predictions        0.371113  0.146808   \n",
       "mse30/bart-base-finetuned-pubmed_semantic_predi...  0.355427  0.140262   \n",
       "facebook/bart-base_semantic_predictions             0.337382  0.113309   \n",
       "facebook/bart-base_predictions                      0.334315  0.114154   \n",
       "facebook/bart-large_semantic_predictions            0.314529  0.091228   \n",
       "facebook/bart-large_predictions                     0.309903  0.086964   \n",
       "google/pegasus-x-large_predictions                  0.301847  0.086326   \n",
       "google/pegasus-x-large_semantic_predictions         0.289379  0.086078   \n",
       "gpt2-medium_predictions                             0.188879  0.019092   \n",
       "gpt2_predictions                                    0.184496  0.017993   \n",
       "t5-small_predictions                                0.181466  0.062812   \n",
       "gpt2_semantic_predictions                           0.176464  0.016244   \n",
       "t5-small_semantic_predictions                       0.173886  0.054415   \n",
       "t5-base_predictions                                 0.169984  0.057185   \n",
       "t5-base_semantic_predictions                        0.161036  0.048650   \n",
       "gayanin/t5-small-finetuned-pubmed_predictions       0.127654  0.049265   \n",
       "gayanin/t5-small-finetuned-pubmed_semantic_pred...  0.117538  0.041906   \n",
       "google/pegasus-x-base_semantic_predictions          0.025700  0.000319   \n",
       "google/pegasus-x-base_predictions                   0.023100  0.000243   \n",
       "\n",
       "                                                      rougeL  rougeLsum  \n",
       "google/bigbird-pegasus-large-pubmed_predictions     0.232430   0.237661  \n",
       "google/pegasus-pubmed_predictions                   0.232697   0.237762  \n",
       "google/pegasus-pubmed_semantic_predictions          0.236825   0.240366  \n",
       "google/bigbird-pegasus-large-pubmed_semantic_pr...  0.223682   0.227538  \n",
       "mse30/bart-base-finetuned-pubmed_predictions        0.228494   0.252258  \n",
       "mse30/bart-base-finetuned-pubmed_semantic_predi...  0.220893   0.246486  \n",
       "facebook/bart-base_semantic_predictions             0.192816   0.196912  \n",
       "facebook/bart-base_predictions                      0.192467   0.196672  \n",
       "facebook/bart-large_semantic_predictions            0.172488   0.176876  \n",
       "facebook/bart-large_predictions                     0.172292   0.175652  \n",
       "google/pegasus-x-large_predictions                  0.163246   0.169236  \n",
       "google/pegasus-x-large_semantic_predictions         0.157694   0.161319  \n",
       "gpt2-medium_predictions                             0.099106   0.120994  \n",
       "gpt2_predictions                                    0.099412   0.120907  \n",
       "t5-small_predictions                                0.129782   0.130728  \n",
       "gpt2_semantic_predictions                           0.099743   0.119662  \n",
       "t5-small_semantic_predictions                       0.119856   0.121929  \n",
       "t5-base_predictions                                 0.122764   0.123243  \n",
       "t5-base_semantic_predictions                        0.112587   0.113431  \n",
       "gayanin/t5-small-finetuned-pubmed_predictions       0.100367   0.101464  \n",
       "gayanin/t5-small-finetuned-pubmed_semantic_pred...  0.092779   0.093721  \n",
       "google/pegasus-x-base_semantic_predictions          0.025306   0.025338  \n",
       "google/pegasus-x-base_predictions                   0.021740   0.021687  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ROUGE_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d8064d8-f148-4660-ac75-adc27d72fe46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time(seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google/pegasus-pubmed_semantic_predictions</th>\n",
       "      <td>7807.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-pubmed_predictions</th>\n",
       "      <td>7218.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/bigbird-pegasus-large-pubmed_semantic_predictions</th>\n",
       "      <td>7007.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-medium_predictions</th>\n",
       "      <td>5967.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/bigbird-pegasus-large-pubmed_predictions</th>\n",
       "      <td>5845.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large_semantic_predictions</th>\n",
       "      <td>4702.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2_semantic_predictions</th>\n",
       "      <td>4397.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large_predictions</th>\n",
       "      <td>4316.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-x-large_semantic_predictions</th>\n",
       "      <td>4139.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-x-base_semantic_predictions</th>\n",
       "      <td>3949.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-x-large_predictions</th>\n",
       "      <td>3507.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google/pegasus-x-base_predictions</th>\n",
       "      <td>3348.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2_predictions</th>\n",
       "      <td>3346.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-base_semantic_predictions</th>\n",
       "      <td>3199.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-base_predictions</th>\n",
       "      <td>2914.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse30/bart-base-finetuned-pubmed_semantic_predictions</th>\n",
       "      <td>1963.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5-base_semantic_predictions</th>\n",
       "      <td>1962.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5-base_predictions</th>\n",
       "      <td>1832.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse30/bart-base-finetuned-pubmed_predictions</th>\n",
       "      <td>1758.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5-small_semantic_predictions</th>\n",
       "      <td>1097.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5-small_predictions</th>\n",
       "      <td>1043.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gayanin/t5-small-finetuned-pubmed_semantic_predictions</th>\n",
       "      <td>704.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gayanin/t5-small-finetuned-pubmed_predictions</th>\n",
       "      <td>662.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    time(seconds)\n",
       "google/pegasus-pubmed_semantic_predictions                7807.32\n",
       "google/pegasus-pubmed_predictions                         7218.26\n",
       "google/bigbird-pegasus-large-pubmed_semantic_pr...        7007.86\n",
       "gpt2-medium_predictions                                   5967.51\n",
       "google/bigbird-pegasus-large-pubmed_predictions           5845.54\n",
       "facebook/bart-large_semantic_predictions                  4702.63\n",
       "gpt2_semantic_predictions                                 4397.33\n",
       "facebook/bart-large_predictions                           4316.24\n",
       "google/pegasus-x-large_semantic_predictions               4139.57\n",
       "google/pegasus-x-base_semantic_predictions                3949.54\n",
       "google/pegasus-x-large_predictions                        3507.90\n",
       "google/pegasus-x-base_predictions                         3348.04\n",
       "gpt2_predictions                                          3346.57\n",
       "facebook/bart-base_semantic_predictions                   3199.57\n",
       "facebook/bart-base_predictions                            2914.38\n",
       "mse30/bart-base-finetuned-pubmed_semantic_predi...        1963.67\n",
       "t5-base_semantic_predictions                              1962.72\n",
       "t5-base_predictions                                       1832.34\n",
       "mse30/bart-base-finetuned-pubmed_predictions              1758.85\n",
       "t5-small_semantic_predictions                             1097.52\n",
       "t5-small_predictions                                      1043.54\n",
       "gayanin/t5-small-finetuned-pubmed_semantic_pred...         704.10\n",
       "gayanin/t5-small-finetuned-pubmed_predictions              662.66"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(TIME_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5394250-0622-4bc6-a4b0-1587b2eb7fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
