{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2517265b-ddff-4aea-a7f4-a4559df25c89",
   "metadata": {},
   "source": [
    "# Finetuning Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8061b90-3283-4325-a78f-99c9a3b64df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# install necessary libraries\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89815cd2-9c04-4f2d-84dd-6f838fdff17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset,DatasetDict, Dataset\n",
    "from transformers import pipeline,DataCollatorForSeq2Seq,AutoModelForSeq2SeqLM, AutoTokenizer,TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b67a56-a182-4da2-bad6-ba36b624cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the gpu device as gpu\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4802d01-20a5-4066-9b91-b81ff4e8928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model checkpoint and load tokenizer and model\n",
    "MODEL_CHECKPOINT = 'facebook/bart-base'\n",
    "# download data using https://drive.google.com/file/d/1tuiAVkfy_EVM0zavMcWPvqeuEGDdpbnG/view?usp=sharing\n",
    "INPUT_DATA_PATH = './finetune_data.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20183a-e5ea-4d2d-b9f8-7bfd71aec53b",
   "metadata": {},
   "source": [
    "## Load Pubmed Dataset\n",
    "Load data into a dataframe with article_id, article_text and article_text_raw as attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcbf8631-3c49-4c4e-9312-1eacd0606bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path:str=INPUT_DATA_PATH)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Pubmed dataset to dataframe\n",
    "    \"\"\"\n",
    "    pubmed_data={}\n",
    "    article_id=\"\"\n",
    "    article_text=\"\"\n",
    "    print(\"############# Started Data Loading ##############\")\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            article_id = json.loads(line)['article_id']\n",
    "            article_text = \" \".join(json.loads(line)['article_text'])\n",
    "            abstract_text=\"\".join(x.strip(\"<S> </S>\") for x in json.loads(line)['abstract_text'])\n",
    "            pubmed_data[article_id]=[article_text,abstract_text]\n",
    "    df=pd.DataFrame.from_dict(pubmed_data,orient='index',columns=['article_text','abstract_text'])\n",
    "    print(\"############# Finished Data Loading ##############\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49670ad9-a2af-4429-b1f7-72a5c00c1dfa",
   "metadata": {},
   "source": [
    "## Preprocess Data to convert it from Dataframe to DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a1fb8ab-3278-4727-a7dc-c6091ab6d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_data():\n",
    "    columns = ['input_ids', 'labels', 'attention_mask']\n",
    "    TRAIN_RANGE = 1000\n",
    "    VAL_RANGE = 1125\n",
    "    TEST_RANGE = 1250\n",
    "    TOKENIZER=AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    def get_feature(self,batch:DatasetDict)->dict:\n",
    "        \"\"\"\n",
    "        Get encodings for all the input data\n",
    "        \"\"\"\n",
    "        encodings = self.TOKENIZER(batch['article_text'], text_target=batch['abstract_text'],max_length=512, truncation=True)\n",
    "        encodings = {'input_ids': encodings['input_ids'],'attention_mask': encodings['attention_mask'],'labels': encodings['labels']}\n",
    "        return encodings\n",
    "        \n",
    "    def map_data(self,pubmed_df:pd.DataFrame)->DatasetDict:\n",
    "        \"\"\"\n",
    "        Map the input data to be compatible with pytorch format\n",
    "        \"\"\"\n",
    "        data_dict = {\n",
    "        \"train\": Dataset.from_dict({\"article_text\": pubmed_df['article_text'].to_list()[:self.TRAIN_RANGE], \"abstract_text\": pubmed_df['abstract_text'].to_list()[:self.TRAIN_RANGE]}),\n",
    "        \"validation\": Dataset.from_dict({\"article_text\": pubmed_df['article_text'].to_list()[self.TRAIN_RANGE:self.VAL_RANGE], \"abstract_text\": pubmed_df['abstract_text'].to_list()[self.TRAIN_RANGE:self.VAL_RANGE]}),\n",
    "        \"test\": Dataset.from_dict({\"article_text\": pubmed_df['article_text'].to_list()[self.VAL_RANGE:self.TEST_RANGE], \"abstract_text\": pubmed_df['abstract_text'].to_list()[self.VAL_RANGE:self.TEST_RANGE]})\n",
    "        }\n",
    "        dataset_dict = DatasetDict(data_dict)\n",
    "        pubmed_pt = dataset_dict.map(self.get_feature, batched=True)\n",
    "        pubmed_pt.set_format(type='torch', columns=self.columns)\n",
    "        return pubmed_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c9eb4-4641-4829-b54f-a24fcfc8ece6",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc6fc1ff-fe89-4bc9-b25d-02edb39ebed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer_class:\n",
    "    def __init__(self,pubmed_pt):\n",
    "        \"\"\"\n",
    "        initialize arguments for training\n",
    "        \"\"\"\n",
    "        self.MODEL=AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "        self.TOKENIZER=AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "        self.DATA_COLLATOR = DataCollatorForSeq2Seq(self.TOKENIZER, model=self.MODEL)\n",
    "        self.TRAINING_ARGS = TrainingArguments(\n",
    "            output_dir = MODEL_CHECKPOINT,\n",
    "            num_train_epochs=10,\n",
    "            warmup_steps = 500,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            weight_decay = 0.01,\n",
    "            logging_steps = 10,\n",
    "            evaluation_strategy = 'steps',\n",
    "            eval_steps=500,\n",
    "            save_steps=1e6,\n",
    "            gradient_accumulation_steps=16\n",
    "            )\n",
    "        self.PUBMED_PT=pubmed_pt\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        train the model and save it locally\n",
    "        \"\"\"\n",
    "        trainer = Trainer(model=self.MODEL, args=self.TRAINING_ARGS, tokenizer=self.TOKENIZER, data_collator=self.DATA_COLLATOR,train_dataset = self.PUBMED_PT['train'], eval_dataset = self.PUBMED_PT['validation'])\n",
    "        trainer.train()\n",
    "        trainer.save_model(MODEL_CHECKPOINT+'_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96587e52-ac4d-4c27-9916-ba3f4e0e2906",
   "metadata": {},
   "source": [
    "## Start finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97ebf67a-c00a-48af-8dfe-a78a7b5b4526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Started Data Loading ##############\n",
      "############# Finished Data Loading ##############\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pubmed_df=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5f4bc1b-7ea9-4ce9-8622-7da20500ca45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04163a04510f4050b2c69c94090b0fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102159e863c6414a9f54ed8fa033ea32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07692b428f94214bda2c8c85c85eec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "pubmed_pt = preprocess_data().map_data(pubmed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56735e6d-5c14-4964-862e-7bc99d7fe2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 18:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=4.360424836476644, metrics={'train_runtime': 1235.9057, 'train_samples_per_second': 0.647, 'train_steps_per_second': 0.01, 'total_flos': 234138799964160.0, 'train_loss': 4.360424836476644, 'epoch': 0.96})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer_class(pubmed_pt).train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68df8a-c139-43ac-8e1a-869e46e9bab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
